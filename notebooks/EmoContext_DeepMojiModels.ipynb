{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: EmoContext\n",
    "\n",
    "Here we have explored models based on DeepMoji embeddings.   \n",
    "**DeepMoji needs to be installed for using it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from deepmoji.model_def import deepmoji_architecture\n",
    "import io\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from deepmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from deepmoji.create_vocab import extend_vocab, VocabBuilder\n",
    "from deepmoji.word_generator import WordGenerator\n",
    "from deepmoji.tokenizer import tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utills import *\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from deepmoji.model_def import deepmoji_feature_encoding\n",
    "from deepmoji.global_variables import PRETRAINED_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in  \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '\n",
    "                line = cSpace.join(lineSplit)\n",
    "\n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "\n",
    "#             conv = ' <eos> '.join(line[1:4])\n",
    "\n",
    "            # Remove any duplicate spaces\n",
    "#             duplicateSpacePattern = re.compile(r'\\ +')\n",
    "#             conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "#             for l in line[1:4]:\n",
    "                \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(line[1:4])\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , conv_train = preprocessData('../data/train.txt', 'test')\n",
    "_ , conv_test = preprocessData('../data/devwithoutlabels.txt', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = conv_train + conv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading deafult voacb\n",
    "with open('./../DeepMoji/model/vocabulary.json', 'r') as f:\n",
    "    vocabulary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv in convs:\n",
    "    for line in conv:\n",
    "        words = tokenize(line)\n",
    "        for word in words:\n",
    "            if word.lower() in vocabulary:\n",
    "                pass\n",
    "            else:\n",
    "                new_words.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5197"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_gen = WordGenerator(list(new_words))\n",
    "vb = VocabBuilder(word_gen)\n",
    "vb.count_all_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "4517\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))\n",
    "print(len(vb.word_counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "extend_vocab(vocabulary, vb, max_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53000\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dict to ../data/new_vocab\n"
     ]
    }
   ],
   "source": [
    "vb.save_vocab(path='../data/new_vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for embedding\n",
      "Loading weights for bi_lstm_0\n",
      "Loading weights for bi_lstm_1\n",
      "Loading weights for attlayer\n",
      "Ignoring weights for softmax\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "deep_feature_extractor = deepmoji_feature_encoding(MAX_LEN, PRETRAINED_PATH)\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        deep_feature_extractor,\n",
    "        input_shape=(3,MAX_LEN)\n",
    "    ))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "# model.add(Dense(2304, activation='relu'))\n",
    "# model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_17 (TimeDis (None, 3, 2304)           22247680  \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               3539456   \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 25,789,188\n",
      "Trainable params: 25,789,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: 1-D Conv-> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# deep_feature_extractor = deepmoji_feature_encoding(MAX_LEN, PRETRAINED_PATH)\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        deep_feature_extractor,\n",
    "        input_shape=(3,MAX_LEN)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(62,return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_18 (TimeDis (None, 3, 2304)           22247680  \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 3, 32)             221216    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 1, 62)             23560     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 1, 62)             0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 62)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 128)               8064      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 22,501,036\n",
      "Trainable params: 22,501,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: 1-D Conv-> Desne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# deep_feature_extractor = deepmoji_feature_encoding(MAX_LEN, PRETRAINED_PATH)\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        deep_feature_extractor,\n",
    "        input_shape=(3,MAX_LEN)\n",
    "    ))\n",
    "def model_cnn():\n",
    "    # 1D Conv Layer with multiple possible kernel sizes\n",
    "    inputs = Input(shape=(3, 2304))\n",
    "\n",
    "    model = Conv1D(filters=300,\n",
    "                   kernel_size=3,\n",
    "                   padding='valid',\n",
    "                   activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(0.001),\n",
    "                   strides=1)(inputs)\n",
    "    \n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "\n",
    "    flat_input = Flatten()(inputs)\n",
    "    flat_input = Dense(512, activation='relu',\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activity_regularizer=regularizers.l2(0.01))(flat_input)\n",
    "    flat_input = Dropout(0.5)(flat_input)\n",
    "\n",
    "\n",
    "    model = Concatenate()([model, flat_input])\n",
    "\n",
    "    model = Dense(264, activation='relu', kernel_regularizer=regularizers.l2(0.01))(model)\n",
    "    model = Dropout(0.3)(model)\n",
    "    predictions = Dense(4, activation='softmax')(model)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(model_cnn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_19 (TimeDis (None, 3, 2304)           22247680  \n",
      "_________________________________________________________________\n",
      "model_4 (Model)              (None, 4)                 5829048   \n",
      "=================================================================\n",
      "Total params: 28,076,728\n",
      "Trainable params: 28,076,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common to all stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_16 (TimeDis (None, 3, 2304)           22247680  \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 3, 200)            2765000   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1, 200)            0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 1, 200)            0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 1, 62)             65224     \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1, 62)             0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 62)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               8064      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 25,086,484\n",
      "Trainable params: 25,086,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X, Y = preprocessData('../data/train.txt',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SentenceTokenizer(vocabulary, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i], infos, stats = st.tokenize_sentences(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30160, 3, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30160, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = to_categorical(Y)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- random remove stopwords\n",
    "- random switch words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [ i for i in range(MAX_LEN)]\n",
    "\n",
    "def switch_words(X):\n",
    "    p = random.random()\n",
    "    if p < 0.3:\n",
    "        new_x = deepcopy(X)\n",
    "        \n",
    "        try:\n",
    "            last_index = np.where(new_x == 0)[0][0]\n",
    "        except:\n",
    "            last_index = MAX_LEN - 1\n",
    "        \n",
    "        first = random.randint(0,last_index) \n",
    "        second = random.randint(0,last_index) \n",
    "        new_x[first],new_x[second] = new_x[second],new_x[first]\n",
    "        return new_x\n",
    "    return X\n",
    "\n",
    "def remove_random_word(X):\n",
    "    p = random.random()\n",
    "    if p < 0.5:\n",
    "        where = random.sample(indexes,1)[0]\n",
    "        where = int(where)\n",
    "        new_x = deepcopy(X)\n",
    "        if new_x[where] is 0:\n",
    "            pass\n",
    "        else:\n",
    "            new_x[where] = 1\n",
    "            return new_x \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, \n",
    "                                                    test_size=0.1, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y,\n",
    "                                                    random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(x_train)\n",
    "y_train = list(y_train)\n",
    "\n",
    "n_sample = len(x_train)\n",
    "for sample in range(n_sample):\n",
    "    new_sample = []\n",
    "    is_new_sample = False\n",
    "    for line in x_train[sample]:\n",
    "        x_1 = switch_words(line)\n",
    "        x_2 = remove_random_word(x_1)\n",
    "        new_sample.append(x_2)\n",
    "        \n",
    "        if x_2 is not line:\n",
    "            is_new_sample = True\n",
    "    \n",
    "    if is_new_sample:\n",
    "        new_sample = np.array(new_sample).reshape(3,MAX_LEN)\n",
    "        x_train.append(new_sample)\n",
    "        y_train.append(y_train[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53154, 3, 10)\n",
      "(53154, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "print(x_train.shape)\n",
    "y_train = np.array(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.01, patience=4, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath='./weights_all_cnn_lstm.hdf5', verbose=1, save_best_only=True, save_weights_only=True, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53154 samples, validate on 3016 samples\n",
      "Epoch 1/2\n",
      "53152/53154 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8608Epoch 00001: val_loss improved from inf to 0.37535, saving model to ./weights_all_cnn_lstm.hdf5\n",
      "53154/53154 [==============================] - 263s 5ms/step - loss: 0.4557 - acc: 0.8608 - val_loss: 0.3753 - val_acc: 0.8966\n",
      "Epoch 2/2\n",
      "53152/53154 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9382Epoch 00002: val_loss did not improve\n",
      "53154/53154 [==============================] - 244s 5ms/step - loss: 0.2405 - acc: 0.9382 - val_loss: 0.4051 - val_acc: 0.8912\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_test,y_test),\n",
    "          shuffle=True,\n",
    "          batch_size=32,\n",
    "          epochs=2,\n",
    "          callbacks=[earlystop,checkpointer],\n",
    "          initial_epoch=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./weights_all_cnn_lstm.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Upload \n",
    "\n",
    "A file called `test.txt` is created which needs to be zipped  and uploaded to the the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_dev = preprocessData('../data/devwithoutlabels.txt',mode='test')\n",
    "\n",
    "st = SentenceTokenizer(vocabulary, MAX_LEN)\n",
    "\n",
    "for i in range(len(x_dev)):\n",
    "    x_dev[i], infos, stats = st.tokenize_sentences(x_dev[i])\n",
    "\n",
    "x_dev = np.array(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2755/2755 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_dev, batch_size=64,verbose=1)\n",
    "predictions = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('../test.txt', \"w\", encoding=\"utf8\") as fout:\n",
    "    fout.write(unicode('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n'))        \n",
    "    with io.open('../data/devwithoutlabels.txt', encoding=\"utf8\") as fin:\n",
    "        fin.readline()\n",
    "        for lineNum, line in enumerate(fin):\n",
    "            fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "            fout.write(unicode(label2emotion[predictions[lineNum]] + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how this looks on the test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3016/3016 [==============================] - 3s 886us/step\n"
     ]
    }
   ],
   "source": [
    "pred_Y = model.predict_classes(x_test)\n",
    "pred_Y = to_categorical(pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 1380.   358.   479.   471.]\n",
      "False Positives per class :  [ 140.   70.   63.   55.]\n",
      "False Negatives per class :  [ 115.   66.   67.   80.]\n",
      "Class happy : Precision : 0.836, Recall : 0.844, F1 : 0.840\n",
      "Class sad : Precision : 0.884, Recall : 0.877, F1 : 0.881\n",
      "Class angry : Precision : 0.895, Recall : 0.855, F1 : 0.875\n",
      "Ignoring the Others class, Macro Precision : 0.8719, Macro Recall : 0.8588, Macro F1 : 0.8653\n",
      "Ignoring the Others class, Micro TP : 1308, FP : 188, FN : 213\n",
      "Accuracy : 0.8912, Micro Precision : 0.8743, Micro Recall : 0.8600, Micro F1 : 0.8671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.89124668435013266,\n",
       " 0.87433155080213909,\n",
       " 0.85996055226824453,\n",
       " 0.86708650977792501)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMetrics(pred_Y,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(predictions, ground):\n",
    "    \"\"\"\n",
    "    FROM: Baseline/starting_kit\n",
    "\n",
    "    Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification\n",
    "    \"\"\"\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "\n",
    "    truePositives = np.sum(discretePredictions * ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground - discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "\n",
    "    # ------------- Macro level calculation ---------------\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, 4):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = (2 * recall * precision) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (label2emotion[c], precision, recall, f1))\n",
    "\n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision) / (macroPrecision + macroRecall) if (\n",
    "                                                                                             macroPrecision + macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (\n",
    "        macroPrecision, macroRecall, macroF1))\n",
    "\n",
    "    # ------------- Micro level calculation ---------------\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()\n",
    "\n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (\n",
    "        truePositives, falsePositives, falseNegatives))\n",
    "\n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)\n",
    "\n",
    "    microF1 = (2 * microRecall * microPrecision) / (microPrecision + microRecall) if (\n",
    "                                                                                             microPrecision + microRecall) > 0 else 0\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    ground = ground.argmax(axis=1)\n",
    "    accuracy = np.mean(predictions == ground)\n",
    "\n",
    "    print(\"Accuracy : %.4f, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (\n",
    "        accuracy, microPrecision, microRecall, microF1))\n",
    "    return accuracy, microPrecision, microRecall, microF1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "```\n",
    "MAX_LEN = 10\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "deep_feature_extractor = deepmoji_feature_encoding(MAX_LEN, PRETRAINED_PATH)\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        deep_feature_extractor,\n",
    "        input_shape=(3,MAX_LEN)\n",
    "    ))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "```_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "time_distributed_2 (TimeDist (None, 3, 2304)           22247680  \n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 6912)              0         \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 6912)              0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 512)               3539456   \n",
    "_________________________________________________________________\n",
    "dropout_4 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 4)                 2052      \n",
    "=================================================================\n",
    "Total params: 25,789,188\n",
    "Trainable params: 25,789,188\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "On 0.1% stratified test\n",
    "```\n",
    "True Positives per class :  [ 1393.   346.   486.   478.]\n",
    "False Positives per class :  [ 145.   44.   71.   53.]\n",
    "False Negatives per class :  [ 102.   78.   60.   73.]\n",
    "Class happy : Precision : 0.887, Recall : 0.816, F1 : 0.850\n",
    "Class sad : Precision : 0.873, Recall : 0.890, F1 : 0.881\n",
    "Class angry : Precision : 0.900, Recall : 0.868, F1 : 0.884\n",
    "Ignoring the Others class, Macro Precision : 0.8866, Macro Recall : 0.8579, Macro F1 : 0.8720\n",
    "Ignoring the Others class, Micro TP : 1310, FP : 168, FN : 211\n",
    "Accuracy : 0.8962, Micro Precision : 0.8863, Micro Recall : 0.8613, Micro F1 : 0.8736\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on embeddings\n",
    "\n",
    "```\n",
    "def model_cnn():\n",
    "    # 1D Conv Layer with multiple possible kernel sizes\n",
    "    inputs = Input(shape=(3, 2304))\n",
    "\n",
    "    model = Conv1D(filters=300,\n",
    "                   kernel_size=3,\n",
    "                   padding='valid',\n",
    "                   activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(0.001),\n",
    "                   strides=1)(inputs)\n",
    "    \n",
    "    model = GlobalMaxPooling1D()(model)\n",
    "\n",
    "    flat_input = Flatten()(inputs)\n",
    "    flat_input = Dense(512, activation='relu',\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activity_regularizer=regularizers.l2(0.01))(flat_input)\n",
    "    flat_input = Dropout(0.5)(flat_input)\n",
    "\n",
    "\n",
    "    model = Concatenate()([model, flat_input])\n",
    "\n",
    "    model = Dense(264, activation='relu', kernel_regularizer=regularizers.l2(0.01))(model)\n",
    "    model = Dropout(0.3)(model)\n",
    "    predictions = Dense(4, activation='softmax')(model)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model\n",
    "    \n",
    " ````\n",
    " \n",
    "On 0.1% stratified test\n",
    "```\n",
    "True Positives per class :  [ 1385.   312.   473.   458.]\n",
    "False Positives per class :  [ 201.   47.   78.   62.]\n",
    "False Negatives per class :  [ 110.  112.   73.   93.]\n",
    "Class happy : Precision : 0.869, Recall : 0.736, F1 : 0.797\n",
    "Class sad : Precision : 0.858, Recall : 0.866, F1 : 0.862\n",
    "Class angry : Precision : 0.881, Recall : 0.831, F1 : 0.855\n",
    "Ignoring the Others class, Macro Precision : 0.8694, Macro Recall : 0.8111, Macro F1 : 0.8393\n",
    "Ignoring the Others class, Micro TP : 1243, FP : 187, FN : 278\n",
    "Accuracy : 0.8714, Micro Precision : 0.8692, Micro Recall : 0.8172, Micro F1 : 0.8424\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN_LSTM Small\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "\n",
    "deep_feature_extractor = deepmoji_feature_encoding(MAX_LEN, PRETRAINED_PATH)\n",
    "\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        deep_feature_extractor,\n",
    "        input_shape=(3,MAX_LEN)\n",
    "    ))\n",
    "\n",
    "model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "```\n",
    "\n",
    "```\n",
    "True Positives per class :  [ 1372.   360.   474.   472.]\n",
    "False Positives per class :  [ 148.   77.   59.   54.]\n",
    "False Negatives per class :  [ 123.   64.   72.   79.]\n",
    "Class happy : Precision : 0.824, Recall : 0.849, F1 : 0.836\n",
    "Class sad : Precision : 0.889, Recall : 0.868, F1 : 0.879\n",
    "Class angry : Precision : 0.897, Recall : 0.857, F1 : 0.877\n",
    "Ignoring the Others class, Macro Precision : 0.8701, Macro Recall : 0.8579, Macro F1 : 0.8640\n",
    "Ignoring the Others class, Micro TP : 1306, FP : 190, FN : 215\n",
    "Accuracy : 0.8879, Micro Precision : 0.8730, Micro Recall : 0.8586, Micro F1 : 0.8658\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
